<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Troubleshooting on KasdevTech</title>
    <link>http://localhost:1313/tags/troubleshooting/</link>
    <description>Recent content in Troubleshooting on KasdevTech</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Terraform Import in Azure — Fixing Out-of-Band Changes</title>
      <link>http://localhost:1313/terraform/out-of-band-changes/</link>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/terraform/out-of-band-changes/</guid>
      <description>
        
          
            Sometimes resources created manually in the Azure Portal need to be managed by Terraform. Here&#39;s how I used `terraform import` to fix drift and sync state.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Fixing High CPU and Memory Usage in Azure App Service</title>
      <link>http://localhost:1313/azure/appservice-high-cpu/</link>
      <pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/azure/appservice-high-cpu/</guid>
      <description>
        
          
            &lt;h4 id=&#34;fixing-high-cpu-and-memory-usage-in-azure-app-service&#34;&gt;Fixing High CPU and Memory Usage in Azure App Service&lt;/h4&gt;
&lt;p&gt;Your app is running fine, and then suddenly…  users complain about &lt;strong&gt;slowness&lt;/strong&gt; or the app even &lt;strong&gt;crashes&lt;/strong&gt;.&lt;br&gt;
When you check the Azure Portal, you see CPU pegged at 90–100% or memory exhausted.&lt;br&gt;
This is a very &lt;strong&gt;common real-world issue&lt;/strong&gt; in Azure App Service. Let’s walk step by step.&lt;/p&gt;
&lt;h4 id=&#34;step-1-monitor-metrics&#34;&gt;Step 1: Monitor Metrics&lt;/h4&gt;
&lt;p&gt;Go to:
&lt;strong&gt;Azure Portal → App Service → Metrics → CPU Percentage / Memory Working Set&lt;/strong&gt;
Look for spikes during:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Troubleshooting Azure App Service Startup Failures</title>
      <link>http://localhost:1313/azure/appservice/</link>
      <pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/azure/appservice/</guid>
      <description>
        
          
            &lt;h4 id=&#34;troubleshooting-azure-app-service-startup-failures&#34;&gt;Troubleshooting Azure App Service Startup Failures&lt;/h4&gt;
&lt;p&gt;One of the most common issues with &lt;strong&gt;Azure App Service&lt;/strong&gt; is when your web app refuses to start after deployment.&lt;/p&gt;
&lt;p&gt;The symptoms usually look like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The app returns &lt;strong&gt;HTTP 500 – Internal Server Error&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You see &lt;strong&gt;Service Unavailable (503)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The container or code doesn’t start properly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a &lt;strong&gt;real-world headache&lt;/strong&gt; because you deploy with confidence, but the app never comes up.&lt;/p&gt;
&lt;h4 id=&#34;step-1-check-app-service-logs&#34;&gt;Step 1: Check App Service Logs&lt;/h4&gt;
&lt;p&gt;Enable &lt;strong&gt;Application Logging&lt;/strong&gt; and &lt;strong&gt;Web Server Logging&lt;/strong&gt;.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Azure Application Gateway – Backend Health Unknown</title>
      <link>http://localhost:1313/azure/appgw_issues/</link>
      <pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/azure/appgw_issues/</guid>
      <description>
        
          
            &lt;h3 id=&#34;azure-application-gateway--backend-health-unknown&#34;&gt;Azure Application Gateway – Backend Health Unknown?&lt;/h3&gt;
&lt;p&gt;If you’re using Azure Application Gateway and you see:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Backend health: UNKNOWN&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Backend health: UNHEALTHY&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Don’t panic. This is a common issue — and we’ll walk through how to fix it &lt;strong&gt;end-to-end&lt;/strong&gt;, even if you’re new to Azure.&lt;/p&gt;
&lt;h4 id=&#34;what-does-unknown-or-unhealthy-mean&#34;&gt;What Does “Unknown” or “Unhealthy” Mean?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unknown&lt;/strong&gt; = App Gateway &lt;strong&gt;can’t reach&lt;/strong&gt; the backend (network issue)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unhealthy&lt;/strong&gt; = App Gateway &lt;strong&gt;can reach&lt;/strong&gt; the backend, but &lt;strong&gt;probe failed&lt;/strong&gt; (wrong path, port, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;step-by-step-troubleshooting&#34;&gt;Step-by-Step Troubleshooting&lt;/h4&gt;
&lt;h4 id=&#34;step-1-check-backend-pool-ipvm&#34;&gt;Step 1: Check Backend Pool IP/VM&lt;/h4&gt;
&lt;p&gt;Go to:
App Gateway → &lt;strong&gt;Backend pools&lt;/strong&gt;
Confirm:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Terraform Role Assignment Fails on Azure — Fixed with Correct RBAC &amp; Automation</title>
      <link>http://localhost:1313/terraform/terraform-role-assigment/</link>
      <pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/terraform/terraform-role-assigment/</guid>
      <description>
        
          
            Terraform often fails when assigning roles in Azure due to RBAC permission issues. Here&#39;s how I fixed this in a CI/CD pipeline using the right roles and automation.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Terraform Destroy Fails in Azure Due to Resource Dependencies</title>
      <link>http://localhost:1313/terraform/terraform-destroy-fails-in-azure/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/terraform/terraform-destroy-fails-in-azure/</guid>
      <description>
        
          
            Running `terraform destroy` in Azure sometimes fails due to implicit resource dependencies. Here&#39;s how I resolved a common NSG and subnet dependency issue and ensured clean teardown.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Terraform State Corruption in Azure — Recovered</title>
      <link>http://localhost:1313/terraform/statefile-corrupted/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/terraform/statefile-corrupted/</guid>
      <description>
        
          
            Faced a corrupted Terraform state file in Azure Blob Storage? Here&#39;s a real-world recovery guide to restore infrastructure state safely in 4 simple steps.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Azure DevOps pipeline stuck on &#39;Initialize job&#39; Step-by-step fix</title>
      <link>http://localhost:1313/devops/pipelinestuck/</link>
      <pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/devops/pipelinestuck/</guid>
      <description>
        
          
            &lt;h4 id=&#34;azure-devops-pipeline-stuck-on-initialize-job-step-by-step-fix&#34;&gt;Azure DevOps pipeline stuck on &amp;lsquo;Initialize job&amp;rsquo;? Step-by-step fix&lt;/h4&gt;
&lt;p&gt;Ever had your Azure DevOps pipeline &lt;strong&gt;hang at the very first step&lt;/strong&gt; – &lt;em&gt;Initialize job&lt;/em&gt; – and not move forward?&lt;/p&gt;
&lt;p&gt;This issue is frustrating and common — especially with &lt;strong&gt;self-hosted agents&lt;/strong&gt;, permissions misconfiguration, or pipeline resource locks.&lt;/p&gt;
&lt;p&gt;Let’s break down exactly how to troubleshoot and fix this problem step-by-step.&lt;/p&gt;
&lt;h4 id=&#34;what-does-initialize-job-actually-mean&#34;&gt;What does &amp;lsquo;Initialize job&amp;rsquo; actually mean?&lt;/h4&gt;
&lt;p&gt;It’s the &lt;strong&gt;very first phase&lt;/strong&gt; in a pipeline run:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Azure DevOps Pipeline Timeout – Fixing Long-Running Build Failures</title>
      <link>http://localhost:1313/devops/pipeline-timeout-issues/</link>
      <pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/devops/pipeline-timeout-issues/</guid>
      <description>
        
          
            &lt;p&gt;Does your build or deployment fail after 60 minutes? You might be hitting a &lt;strong&gt;default timeout&lt;/strong&gt; on Azure Pipelines.&lt;/p&gt;
&lt;h4 id=&#34;error-message&#34;&gt;Error Message&lt;/h4&gt;
&lt;p&gt;[error]The job was canceled after reaching the timeout limit of 60 minutes.&lt;/p&gt;
&lt;h4 id=&#34;default-limits&#34;&gt;Default Limits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Microsoft-hosted agents&lt;/strong&gt;: 60 minutes per job (can be increased)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-hosted agents&lt;/strong&gt;: You control the timeout&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;increase-timeout-in-yaml&#34;&gt;Increase Timeout in YAML&lt;/h4&gt;
&lt;p&gt;Add the &lt;code&gt;timeoutInMinutes&lt;/code&gt; setting to your job:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;jobs&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- &lt;span style=&#34;color:#f92672&#34;&gt;job&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Build&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;timeoutInMinutes&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;pool&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;vmImage&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ubuntu-latest&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;steps&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;script&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;echo &amp;#34;Building...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;optimize-the-build&#34;&gt;Optimize the Build&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Avoid long dependency installations (e.g., move to cache or Docker layer)&lt;/li&gt;
&lt;li&gt;Parallelize jobs using strategy and matrix&lt;/li&gt;
&lt;li&gt;Use restoreCache and saveCache for npm, pip, Maven, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;use-self-hosted-agent-no-hard-timeout&#34;&gt;Use Self-Hosted Agent (No Hard Timeout)&lt;/h4&gt;
&lt;p&gt;Set up a self-hosted agent in Azure VM or AKS to avoid limits altogether.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Azure Application Gateway 502 Error – Full Troubleshooting Guide for Beginners</title>
      <link>http://localhost:1313/azure/application-gateway-502-error/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/azure/application-gateway-502-error/</guid>
      <description>
        
          
            &lt;p&gt;If you’ve seen this before, don’t panic. This guide walks you through:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What a 502 error means in Azure&lt;/li&gt;
&lt;li&gt;How to identify the root cause&lt;/li&gt;
&lt;li&gt;How to fix it with simple step-by-step actions&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;step-by-step-troubleshooting&#34;&gt;Step-by-Step Troubleshooting&lt;/h4&gt;
&lt;h4 id=&#34;step-1-reproduce-the-problem&#34;&gt;Step 1: Reproduce the Problem&lt;/h4&gt;
&lt;p&gt;Go to the App Gateway public IP or custom domain (e.g., &lt;code&gt;https://myappexampleapp.com&lt;/code&gt;)&lt;br&gt;
Check if the site is returning this:
502 - Web server received an invalid response while acting as a gateway or proxy server.
This means App Gateway couldn’t reach your backend &lt;strong&gt;properly&lt;/strong&gt; — not that the app is down.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title> AKS Node Disk Pressure </title>
      <link>http://localhost:1313/az-104/aks-node-disk-pressure/</link>
      <pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/az-104/aks-node-disk-pressure/</guid>
      <description>
        
          
            &lt;h4 id=&#34;what-happened&#34;&gt;What Happened?&lt;/h4&gt;
&lt;p&gt;Hey folks,&lt;br&gt;
Let me share a real incident we faced in production on &lt;strong&gt;Azure Kubernetes Service (AKS)&lt;/strong&gt;. Our workloads were behaving oddly — pods getting evicted, app downtime alerts, and our monitoring tools screaming &lt;code&gt;DiskPressure&lt;/code&gt; on some nodes.&lt;/p&gt;
&lt;p&gt;We didn’t make any infra changes recently, so the obvious question was:&lt;br&gt;
&lt;strong&gt;What’s going on inside the AKS nodes?&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;root-cause-analysis&#34;&gt;Root Cause Analysis&lt;/h4&gt;
&lt;p&gt;We dug into the node metrics using &lt;strong&gt;Azure Monitor&lt;/strong&gt; and &lt;code&gt;kubectl describe node&lt;/code&gt;. Here’s what we found:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title> AKS Node Disk Pressure </title>
      <link>http://localhost:1313/azure/aks-node-disk-pressure/</link>
      <pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/azure/aks-node-disk-pressure/</guid>
      <description>
        
          
            &lt;h4 id=&#34;what-happened&#34;&gt;What Happened?&lt;/h4&gt;
&lt;p&gt;Hey folks,&lt;br&gt;
Let me share a real incident we faced in production on &lt;strong&gt;Azure Kubernetes Service (AKS)&lt;/strong&gt;. Our workloads were behaving oddly — pods getting evicted, app downtime alerts, and our monitoring tools screaming &lt;code&gt;DiskPressure&lt;/code&gt; on some nodes.&lt;/p&gt;
&lt;p&gt;We didn’t make any infra changes recently, so the obvious question was:&lt;br&gt;
&lt;strong&gt;What’s going on inside the AKS nodes?&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;root-cause-analysis&#34;&gt;Root Cause Analysis&lt;/h4&gt;
&lt;p&gt;We dug into the node metrics using &lt;strong&gt;Azure Monitor&lt;/strong&gt; and &lt;code&gt;kubectl describe node&lt;/code&gt;. Here’s what we found:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
