<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Troubleshooting on KasdevTech</title><link>https://kasdevtech.github.io/tags/troubleshooting/</link><description>Recent content in Troubleshooting on KasdevTech</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 01 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://kasdevtech.github.io/tags/troubleshooting/index.xml" rel="self" type="application/rss+xml"/><item><title>Terraform Import in Azure — Fixing Out-of-Band Changes</title><link>https://kasdevtech.github.io/terraform/out-of-band-changes/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/terraform/out-of-band-changes/</guid><description>
Sometimes resources created manually in the Azure Portal need to be managed by Terraform. Here's how I used `terraform import` to fix drift and sync state.</description></item><item><title>Fixing High CPU and Memory Usage in Azure App Service</title><link>https://kasdevtech.github.io/azure/appservice-high-cpu/</link><pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/azure/appservice-high-cpu/</guid><description>
&lt;h4 id="fixing-high-cpu-and-memory-usage-in-azure-app-service">Fixing High CPU and Memory Usage in Azure App Service&lt;/h4>
&lt;p>Your app is running fine, and then suddenly… users complain about &lt;strong>slowness&lt;/strong> or the app even &lt;strong>crashes&lt;/strong>.&lt;br>
When you check the Azure Portal, you see CPU pegged at 90–100% or memory exhausted.&lt;br>
This is a very &lt;strong>common real-world issue&lt;/strong> in Azure App Service. Let’s walk step by step.&lt;/p>
&lt;h4 id="step-1-monitor-metrics">Step 1: Monitor Metrics&lt;/h4>
&lt;p>Go to:
&lt;strong>Azure Portal → App Service → Metrics → CPU Percentage / Memory Working Set&lt;/strong>
Look for spikes during:&lt;/p></description></item><item><title>Troubleshooting Azure App Service Startup Failures</title><link>https://kasdevtech.github.io/azure/appservice/</link><pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/azure/appservice/</guid><description>
&lt;h4 id="troubleshooting-azure-app-service-startup-failures">Troubleshooting Azure App Service Startup Failures&lt;/h4>
&lt;p>One of the most common issues with &lt;strong>Azure App Service&lt;/strong> is when your web app refuses to start after deployment.&lt;/p>
&lt;p>The symptoms usually look like this:&lt;/p>
&lt;ul>
&lt;li>The app returns &lt;strong>HTTP 500 – Internal Server Error&lt;/strong>&lt;/li>
&lt;li>You see &lt;strong>Service Unavailable (503)&lt;/strong>&lt;/li>
&lt;li>The container or code doesn’t start properly&lt;/li>
&lt;/ul>
&lt;p>This is a &lt;strong>real-world headache&lt;/strong> because you deploy with confidence, but the app never comes up.&lt;/p>
&lt;h4 id="step-1-check-app-service-logs">Step 1: Check App Service Logs&lt;/h4>
&lt;p>Enable &lt;strong>Application Logging&lt;/strong> and &lt;strong>Web Server Logging&lt;/strong>.&lt;/p></description></item><item><title>Azure Application Gateway – Backend Health Unknown</title><link>https://kasdevtech.github.io/azure/appgw_issues/</link><pubDate>Sun, 03 Aug 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/azure/appgw_issues/</guid><description>
&lt;h3 id="azure-application-gateway--backend-health-unknown">Azure Application Gateway – Backend Health Unknown?&lt;/h3>
&lt;p>If you’re using Azure Application Gateway and you see:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Backend health: UNKNOWN&lt;/strong>&lt;br>
&lt;strong>Backend health: UNHEALTHY&lt;/strong>&lt;/p>&lt;/blockquote>
&lt;p>Don’t panic. This is a common issue — and we’ll walk through how to fix it &lt;strong>end-to-end&lt;/strong>, even if you’re new to Azure.&lt;/p>
&lt;h4 id="what-does-unknown-or-unhealthy-mean">What Does “Unknown” or “Unhealthy” Mean?&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Unknown&lt;/strong> = App Gateway &lt;strong>can’t reach&lt;/strong> the backend (network issue)&lt;/li>
&lt;li>&lt;strong>Unhealthy&lt;/strong> = App Gateway &lt;strong>can reach&lt;/strong> the backend, but &lt;strong>probe failed&lt;/strong> (wrong path, port, etc.)&lt;/li>
&lt;/ul>
&lt;h4 id="step-by-step-troubleshooting">Step-by-Step Troubleshooting&lt;/h4>
&lt;h4 id="step-1-check-backend-pool-ipvm">Step 1: Check Backend Pool IP/VM&lt;/h4>
&lt;p>Go to:
App Gateway → &lt;strong>Backend pools&lt;/strong>
Confirm:&lt;/p></description></item><item><title>Terraform Role Assignment Fails on Azure — Fixed with Correct RBAC &amp; Automation</title><link>https://kasdevtech.github.io/terraform/terraform-role-assigment/</link><pubDate>Tue, 29 Jul 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/terraform/terraform-role-assigment/</guid><description>
Terraform often fails when assigning roles in Azure due to RBAC permission issues. Here's how I fixed this in a CI/CD pipeline using the right roles and automation.</description></item><item><title>Terraform Destroy Fails in Azure Due to Resource Dependencies</title><link>https://kasdevtech.github.io/terraform/terraform-destroy-fails-in-azure/</link><pubDate>Wed, 16 Jul 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/terraform/terraform-destroy-fails-in-azure/</guid><description>
Running `terraform destroy` in Azure sometimes fails due to implicit resource dependencies. Here's how I resolved a common NSG and subnet dependency issue and ensured clean teardown.</description></item><item><title>Terraform State Corruption in Azure — Recovered</title><link>https://kasdevtech.github.io/terraform/statefile-corrupted/</link><pubDate>Wed, 16 Jul 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/terraform/statefile-corrupted/</guid><description>
Faced a corrupted Terraform state file in Azure Blob Storage? Here's a real-world recovery guide to restore infrastructure state safely in 4 simple steps.</description></item><item><title>Azure DevOps pipeline stuck on 'Initialize job' Step-by-step fix</title><link>https://kasdevtech.github.io/devops/pipelinestuck/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/devops/pipelinestuck/</guid><description>
&lt;h4 id="azure-devops-pipeline-stuck-on-initialize-job-step-by-step-fix">Azure DevOps pipeline stuck on &amp;lsquo;Initialize job&amp;rsquo;? Step-by-step fix&lt;/h4>
&lt;p>Ever had your Azure DevOps pipeline &lt;strong>hang at the very first step&lt;/strong> – &lt;em>Initialize job&lt;/em> – and not move forward?&lt;/p>
&lt;p>This issue is frustrating and common — especially with &lt;strong>self-hosted agents&lt;/strong>, permissions misconfiguration, or pipeline resource locks.&lt;/p>
&lt;p>Let’s break down exactly how to troubleshoot and fix this problem step-by-step.&lt;/p>
&lt;h4 id="what-does-initialize-job-actually-mean">What does &amp;lsquo;Initialize job&amp;rsquo; actually mean?&lt;/h4>
&lt;p>It’s the &lt;strong>very first phase&lt;/strong> in a pipeline run:&lt;/p></description></item><item><title>Azure DevOps Pipeline Timeout – Fixing Long-Running Build Failures</title><link>https://kasdevtech.github.io/devops/pipeline-timeout-issues/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/devops/pipeline-timeout-issues/</guid><description>
&lt;p>Does your build or deployment fail after 60 minutes? You might be hitting a &lt;strong>default timeout&lt;/strong> on Azure Pipelines.&lt;/p>
&lt;h4 id="error-message">Error Message&lt;/h4>
&lt;p>[error]The job was canceled after reaching the timeout limit of 60 minutes.&lt;/p>
&lt;h4 id="default-limits">Default Limits&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Microsoft-hosted agents&lt;/strong>: 60 minutes per job (can be increased)&lt;/li>
&lt;li>&lt;strong>Self-hosted agents&lt;/strong>: You control the timeout&lt;/li>
&lt;/ul>
&lt;h4 id="increase-timeout-in-yaml">Increase Timeout in YAML&lt;/h4>
&lt;p>Add the &lt;code>timeoutInMinutes&lt;/code> setting to your job:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">jobs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">job&lt;/span>: &lt;span style="color:#ae81ff">Build&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">timeoutInMinutes&lt;/span>: &lt;span style="color:#ae81ff">120&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">pool&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">vmImage&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;ubuntu-latest&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">steps&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">script&lt;/span>: &lt;span style="color:#ae81ff">echo &amp;#34;Building...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="optimize-the-build">Optimize the Build&lt;/h4>
&lt;ul>
&lt;li>Avoid long dependency installations (e.g., move to cache or Docker layer)&lt;/li>
&lt;li>Parallelize jobs using strategy and matrix&lt;/li>
&lt;li>Use restoreCache and saveCache for npm, pip, Maven, etc.&lt;/li>
&lt;/ul>
&lt;h4 id="use-self-hosted-agent-no-hard-timeout">Use Self-Hosted Agent (No Hard Timeout)&lt;/h4>
&lt;p>Set up a self-hosted agent in Azure VM or AKS to avoid limits altogether.&lt;/p></description></item><item><title>Azure Application Gateway 502 Error – Full Troubleshooting Guide for Beginners</title><link>https://kasdevtech.github.io/azure/application-gateway-502-error/</link><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/azure/application-gateway-502-error/</guid><description>
&lt;p>If you’ve seen this before, don’t panic. This guide walks you through:&lt;/p>
&lt;ul>
&lt;li>What a 502 error means in Azure&lt;/li>
&lt;li>How to identify the root cause&lt;/li>
&lt;li>How to fix it with simple step-by-step actions&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="step-by-step-troubleshooting">Step-by-Step Troubleshooting&lt;/h4>
&lt;h4 id="step-1-reproduce-the-problem">Step 1: Reproduce the Problem&lt;/h4>
&lt;p>Go to the App Gateway public IP or custom domain (e.g., &lt;code>https://myappexampleapp.com&lt;/code>)&lt;br>
Check if the site is returning this:
502 - Web server received an invalid response while acting as a gateway or proxy server.
This means App Gateway couldn’t reach your backend &lt;strong>properly&lt;/strong> — not that the app is down.&lt;/p></description></item><item><title> AKS Node Disk Pressure</title><link>https://kasdevtech.github.io/azure/aks-node-disk-pressure/</link><pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate><guid>https://kasdevtech.github.io/azure/aks-node-disk-pressure/</guid><description>
&lt;h4 id="what-happened">What Happened?&lt;/h4>
&lt;p>Hey folks,&lt;br>
Let me share a real incident we faced in production on &lt;strong>Azure Kubernetes Service (AKS)&lt;/strong>. Our workloads were behaving oddly — pods getting evicted, app downtime alerts, and our monitoring tools screaming &lt;code>DiskPressure&lt;/code> on some nodes.&lt;/p>
&lt;p>We didn’t make any infra changes recently, so the obvious question was:&lt;br>
&lt;strong>What’s going on inside the AKS nodes?&lt;/strong>&lt;/p>
&lt;h4 id="root-cause-analysis">Root Cause Analysis&lt;/h4>
&lt;p>We dug into the node metrics using &lt;strong>Azure Monitor&lt;/strong> and &lt;code>kubectl describe node&lt;/code>. Here’s what we found:&lt;/p></description></item></channel></rss>